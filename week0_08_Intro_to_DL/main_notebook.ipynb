{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Practice: Basic Artificial Neural Networks\n",
        "Credits: this notebook belongs to [Practical DL](https://docs.google.com/forms/d/e/1FAIpQLScvrVtuwrHSlxWqHnLt1V-_7h2eON_mlRR6MUb3xEe5x9LuoA/viewform?usp\u003dsf_link) course by Yandex School of Data Analysis.\n",
        "\n",
        "We will start working with neural networks on the practice session. Your homework will be to finish the implementation of the layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Our goal is simple, yet an actual implementation may take some time :). We are going to write an Artificial Neural Network (almost) from scratch. The software design was heavily inspired by [PyTorch](http://pytorch.org) which is the main framework of our course "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Speaking about the homework (once again, it will be really similar to this seminar), it requires sending **multiple** files, please do not forget to include all the files when sending to TA. The list of files:\n",
        "- This notebook\n",
        "- modules.ipynb with all blocks implemented (except maybe `Conv2d` and `MaxPool2d` layers implementation which are part of \u0027advanced\u0027 version of this homework)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from time import time, sleep\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Framework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Implement everything in `modules.ipynb`. Read all the comments thoughtfully to ease the pain. Please try not to change the prototypes.\n",
        "\n",
        "Do not forget, that each module should return **AND** store `output` and `gradInput`.\n",
        "\n",
        "The typical assumption is that `module.backward` is always executed after `module.forward`,\n",
        "so `output` is stored, this would be useful for `SoftMax`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Tech note\n",
        "Prefer using `np.multiply`, `np.add`, `np.divide`, `np.subtract` instead of `*`,`+`,`/`,`-` for better memory handling.\n",
        "\n",
        "Example: suppose you allocated a variable \n",
        "\n",
        "```\n",
        "a \u003d np.zeros(...)\n",
        "```\n",
        "So, instead of\n",
        "```\n",
        "a \u003d b + c  # will be reallocated, GC needed to free\n",
        "``` \n",
        "You can use: \n",
        "```\n",
        "np.add(b,c,out \u003d a) # puts result in `a`\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m~\\PycharmProjects\\ml-mipt\\week0_08_Intro_to_DL\\modules.ipynb\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----\u003e 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mskimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named \u0027skimage\u0027"
          ],
          "ename": "ModuleNotFoundError",
          "evalue": "No module named \u0027skimage\u0027",
          "output_type": "error"
        }
      ],
      "source": [
        "# (re-)load layers\n",
        "%run modules.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Toy example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Use this example to debug your code, start with logistic regression and then test other layers. You do not need to change anything here. This code is provided for you to test the layers. Also it is easy to use this code in MNIST task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# Generate some data\n",
        "N \u003d 500\n",
        "\n",
        "X1 \u003d np.random.randn(N,2) + np.array([2,2])\n",
        "X2 \u003d np.random.randn(N,2) + np.array([-2,-2])\n",
        "\n",
        "Y \u003d np.concatenate([np.ones(N),np.zeros(N)])[:,None]\n",
        "Y \u003d np.hstack([Y, 1-Y])\n",
        "\n",
        "X \u003d np.vstack([X1,X2])\n",
        "plt.scatter(X[:,0],X[:,1], c \u003d Y[:,0], edgecolors\u003d \u0027none\u0027)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Define a **logistic regression** for debugging. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "net \u003d Sequential()\n",
        "net.add(Linear(2, 2))\n",
        "net.add(LogSoftMax())\n",
        "\n",
        "criterion \u003d ClassNLLCriterion()\n",
        "\n",
        "print(net)\n",
        "\n",
        "# Test something like that then \n",
        "\n",
        "# net \u003d Sequential()\n",
        "# net.add(Linear(2, 4))\n",
        "# net.add(ReLU())\n",
        "# net.add(Linear(4, 2))\n",
        "# net.add(LogSoftMax())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Start with batch_size \u003d 1000 to make sure every step lowers the loss, then try stochastic version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# Iptimizer params\n",
        "optimizer_config \u003d {\u0027learning_rate\u0027 : 1e-1, \u0027momentum\u0027: 0.9}\n",
        "optimizer_state \u003d {}\n",
        "\n",
        "# Looping params\n",
        "n_epoch \u003d 20\n",
        "batch_size \u003d 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# batch generator\n",
        "def get_batches(dataset, batch_size):\n",
        "    X, Y \u003d dataset\n",
        "    n_samples \u003d X.shape[0]\n",
        "        \n",
        "    # Shuffle at the start of epoch\n",
        "    indices \u003d np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end \u003d min(start + batch_size, n_samples)\n",
        "        \n",
        "        batch_idx \u003d indices[start:end]\n",
        "    \n",
        "        yield X[batch_idx], Y[batch_idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Basic training loop. Examine it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "loss_history \u003d []\n",
        "\n",
        "for i in range(n_epoch):\n",
        "    for x_batch, y_batch in get_batches((X, Y), batch_size):\n",
        "        \n",
        "        net.zeroGradParameters()\n",
        "        \n",
        "        # Forward\n",
        "        predictions \u003d net.forward(x_batch)\n",
        "        loss \u003d criterion.forward(predictions, y_batch)\n",
        "    \n",
        "        # Backward\n",
        "        dp \u003d criterion.backward(predictions, y_batch)\n",
        "        net.backward(x_batch, dp)\n",
        "        \n",
        "        # Update weights\n",
        "        sgd_momentum(net.getParameters(), \n",
        "                     net.getGradParameters(), \n",
        "                     optimizer_config,\n",
        "                     optimizer_state)      \n",
        "        \n",
        "        loss_history.append(loss)\n",
        "\n",
        "    # Visualize\n",
        "    display.clear_output(wait\u003dTrue)\n",
        "    plt.figure(figsize\u003d(8, 6))\n",
        "        \n",
        "    plt.title(\"Training loss\")\n",
        "    plt.xlabel(\"#iteration\")\n",
        "    plt.ylabel(\"loss\")\n",
        "    plt.plot(loss_history, \u0027b\u0027)\n",
        "    plt.show()\n",
        "    \n",
        "    print(\u0027Current loss: %f\u0027 % loss)    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Digit classification "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "We are using old good [MNIST](http://yann.lecun.com/exdb/mnist/) as our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "import mnist\n",
        "X_train, y_train, X_val, y_val, X_test, y_test \u003d mnist.load_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "One-hot encode the labels first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# Your code goes here. ################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "- **Compare** `ReLU`, `ELU`, `LeakyReLU`, `SoftPlus` activation functions. \n",
        "You would better pick the best optimizer params for each of them, but it is overkill for now. Use an architecture of your choice for the comparison.\n",
        "- **Try** inserting `BatchNormalization` (folowed by `ChannelwiseScaling`) between `Linear` module and activation functions.\n",
        "- Plot the losses both from activation functions comparison and `BatchNormalization` comparison on one plot. Please find a scale (log?) when the lines are distinguishable, do not forget about naming the axes, the plot should be goodlooking.\n",
        "- Plot the losses for two networks: one trained by momentum_sgd, another one trained by Adam. Which one performs better?\n",
        "- Hint: good logloss for MNIST should be around 0.5. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# Your code goes here. ################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Write your personal opinion on the activation functions, think about computation times too. Does `BatchNormalization` help?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# Your answer goes here. ################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "**Finally**, use all your knowledge to build a super cool model on this dataset. Use **dropout** to prevent overfitting, play with **learning rate decay**. You can use **data augmentation** such as rotations, translations to boost your score. Use your knowledge and imagination to train a model. Don\u0027t forget to call `training()` and `evaluate()` methods to set desired behaviour of `BatchNormalization` and `Dropout` layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# Your code goes here. ################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Print here your accuracy on test set. It should be around 90%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# Your answer goes here. ################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Comparing with PyTorch implementation\n",
        "The last (and maybe the easiest step after compared to the previous tasks: build a network with the same architecture as above now with PyTorch.\n",
        "\n",
        "You can refer to the `week0_09` or `Lab3_part2` notebooks for hints.\n",
        "\n",
        "__Good Luck!__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# Your beautiful code here."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Create Assignment",
    "kernelspec": {
      "display_name": "Py3 research env",
      "language": "python",
      "name": "py3_research"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "Copy of Lab1_part2_ml_pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-86e0de040aac317a",
          "locked": true,
          "schema_version": 2,
          "solution": false
        },
        "id": "O-4Pd08TY0rG",
        "colab_type": "text"
      },
      "source": [
        "# Lab assignment №1, part 2\n",
        "\n",
        "This lab assignment consists of several parts. You are supposed to make some transformations, train some models, estimate the quality of the models and explain your results.\n",
        "\n",
        "Several comments:\n",
        "* Don't hesitate to ask questions, it's a good practice.\n",
        "* No private/public sharing, please. The copied assignments will be graded with 0 points.\n",
        "* Blocks of this lab will be graded separately."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4eChL4QY0rL",
        "colab_type": "text"
      },
      "source": [
        "__*This is the second part of the assignment. First and third parts are waiting for you in the same directory.*__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-512ba712fc0fc065",
          "locked": true,
          "schema_version": 2,
          "solution": false
        },
        "id": "xIguM-zOY0rO",
        "colab_type": "text"
      },
      "source": [
        "## Part 2. Data preprocessing, model training and evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-b656a4266174b009",
          "locked": true,
          "schema_version": 2,
          "solution": false
        },
        "id": "wKn7P_SkY0rT",
        "colab_type": "text"
      },
      "source": [
        "### 1. Reading the data\n",
        "Today we work with the [dataset](https://archive.ics.uci.edu/ml/datasets/Statlog+%28Vehicle+Silhouettes%29), describing different cars for multiclass ($k=4$) classification problem. The data is available below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdYIQMxOY0rV",
        "colab_type": "code",
        "outputId": "08bf3610-ad2e-4142-893b-50582651537f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "# If on colab, uncomment the following lines\n",
        "\n",
        "! wget https://raw.githubusercontent.com/ml-mipt/ml-mipt/basic_s20/homeworks_basic/Lab1_ML_pipeline_and_SVM/car_data.csv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-25 17:22:42--  https://raw.githubusercontent.com/ml-mipt/ml-mipt/basic_s20/homeworks_basic/Lab1_ML_pipeline_and_SVM/car_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 58374 (57K) [text/plain]\n",
            "Saving to: ‘car_data.csv’\n",
            "\n",
            "\rcar_data.csv          0%[                    ]       0  --.-KB/s               \rcar_data.csv        100%[===================>]  57.01K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2020-03-25 17:22:43 (2.27 MB/s) - ‘car_data.csv’ saved [58374/58374]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-eebac6bfdf73d0bc",
          "locked": true,
          "schema_version": 2,
          "solution": false
        },
        "id": "k7ZvsSs_Y0rg",
        "colab_type": "code",
        "outputId": "76ebbf0b-2a0f-4f40-86f7-316c88713b1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dataset = pd.read_csv('car_data.csv', delimiter=',', header=None).values\n",
        "data = dataset[:, :-1].astype(int)\n",
        "target = dataset[:, -1]\n",
        "\n",
        "print(data.shape, target.shape)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.35)\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(846, 19) (846,)\n",
            "(549, 19) (549,) (297, 19) (297,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-88b1a0f688568f2c",
          "locked": true,
          "schema_version": 2,
          "solution": false
        },
        "id": "8qtVvWAPY0rq",
        "colab_type": "text"
      },
      "source": [
        "To get some insights about the dataset, `pandas` might be used. The `train` part is transformed to `pd.DataFrame` below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmlrRoB-Y0rs",
        "colab_type": "code",
        "outputId": "89f9755b-99ac-4355-86fd-81306591293d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        }
      },
      "source": [
        "X_train_pd = pd.DataFrame(X_train)\n",
        "\n",
        "# First 15 rows of our dataset.\n",
        "X_train_pd.head(15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>383</td>\n",
              "      <td>100</td>\n",
              "      <td>51</td>\n",
              "      <td>109</td>\n",
              "      <td>224</td>\n",
              "      <td>67</td>\n",
              "      <td>9</td>\n",
              "      <td>217</td>\n",
              "      <td>30</td>\n",
              "      <td>24</td>\n",
              "      <td>162</td>\n",
              "      <td>238</td>\n",
              "      <td>704</td>\n",
              "      <td>206</td>\n",
              "      <td>72</td>\n",
              "      <td>6</td>\n",
              "      <td>18</td>\n",
              "      <td>189</td>\n",
              "      <td>199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>47</td>\n",
              "      <td>85</td>\n",
              "      <td>42</td>\n",
              "      <td>66</td>\n",
              "      <td>122</td>\n",
              "      <td>54</td>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>46</td>\n",
              "      <td>19</td>\n",
              "      <td>141</td>\n",
              "      <td>172</td>\n",
              "      <td>317</td>\n",
              "      <td>174</td>\n",
              "      <td>88</td>\n",
              "      <td>6</td>\n",
              "      <td>14</td>\n",
              "      <td>180</td>\n",
              "      <td>182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>319</td>\n",
              "      <td>102</td>\n",
              "      <td>51</td>\n",
              "      <td>92</td>\n",
              "      <td>194</td>\n",
              "      <td>60</td>\n",
              "      <td>6</td>\n",
              "      <td>220</td>\n",
              "      <td>30</td>\n",
              "      <td>25</td>\n",
              "      <td>162</td>\n",
              "      <td>247</td>\n",
              "      <td>731</td>\n",
              "      <td>209</td>\n",
              "      <td>80</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>188</td>\n",
              "      <td>186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>675</td>\n",
              "      <td>100</td>\n",
              "      <td>58</td>\n",
              "      <td>109</td>\n",
              "      <td>230</td>\n",
              "      <td>70</td>\n",
              "      <td>11</td>\n",
              "      <td>226</td>\n",
              "      <td>30</td>\n",
              "      <td>25</td>\n",
              "      <td>182</td>\n",
              "      <td>234</td>\n",
              "      <td>752</td>\n",
              "      <td>207</td>\n",
              "      <td>72</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>187</td>\n",
              "      <td>198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>667</td>\n",
              "      <td>110</td>\n",
              "      <td>53</td>\n",
              "      <td>104</td>\n",
              "      <td>223</td>\n",
              "      <td>66</td>\n",
              "      <td>10</td>\n",
              "      <td>211</td>\n",
              "      <td>32</td>\n",
              "      <td>24</td>\n",
              "      <td>164</td>\n",
              "      <td>223</td>\n",
              "      <td>659</td>\n",
              "      <td>210</td>\n",
              "      <td>67</td>\n",
              "      <td>5</td>\n",
              "      <td>16</td>\n",
              "      <td>190</td>\n",
              "      <td>203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>539</td>\n",
              "      <td>109</td>\n",
              "      <td>55</td>\n",
              "      <td>96</td>\n",
              "      <td>191</td>\n",
              "      <td>57</td>\n",
              "      <td>6</td>\n",
              "      <td>241</td>\n",
              "      <td>28</td>\n",
              "      <td>26</td>\n",
              "      <td>170</td>\n",
              "      <td>267</td>\n",
              "      <td>857</td>\n",
              "      <td>242</td>\n",
              "      <td>85</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "      <td>184</td>\n",
              "      <td>184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>789</td>\n",
              "      <td>90</td>\n",
              "      <td>39</td>\n",
              "      <td>85</td>\n",
              "      <td>160</td>\n",
              "      <td>59</td>\n",
              "      <td>7</td>\n",
              "      <td>163</td>\n",
              "      <td>41</td>\n",
              "      <td>20</td>\n",
              "      <td>131</td>\n",
              "      <td>189</td>\n",
              "      <td>396</td>\n",
              "      <td>158</td>\n",
              "      <td>71</td>\n",
              "      <td>7</td>\n",
              "      <td>13</td>\n",
              "      <td>186</td>\n",
              "      <td>192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>409</td>\n",
              "      <td>86</td>\n",
              "      <td>38</td>\n",
              "      <td>86</td>\n",
              "      <td>175</td>\n",
              "      <td>60</td>\n",
              "      <td>9</td>\n",
              "      <td>170</td>\n",
              "      <td>39</td>\n",
              "      <td>21</td>\n",
              "      <td>134</td>\n",
              "      <td>191</td>\n",
              "      <td>433</td>\n",
              "      <td>138</td>\n",
              "      <td>68</td>\n",
              "      <td>1</td>\n",
              "      <td>28</td>\n",
              "      <td>191</td>\n",
              "      <td>199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>121</td>\n",
              "      <td>90</td>\n",
              "      <td>48</td>\n",
              "      <td>78</td>\n",
              "      <td>142</td>\n",
              "      <td>59</td>\n",
              "      <td>11</td>\n",
              "      <td>160</td>\n",
              "      <td>43</td>\n",
              "      <td>20</td>\n",
              "      <td>160</td>\n",
              "      <td>173</td>\n",
              "      <td>370</td>\n",
              "      <td>185</td>\n",
              "      <td>76</td>\n",
              "      <td>10</td>\n",
              "      <td>11</td>\n",
              "      <td>183</td>\n",
              "      <td>192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>702</td>\n",
              "      <td>96</td>\n",
              "      <td>48</td>\n",
              "      <td>83</td>\n",
              "      <td>177</td>\n",
              "      <td>59</td>\n",
              "      <td>8</td>\n",
              "      <td>171</td>\n",
              "      <td>39</td>\n",
              "      <td>21</td>\n",
              "      <td>152</td>\n",
              "      <td>195</td>\n",
              "      <td>438</td>\n",
              "      <td>196</td>\n",
              "      <td>67</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>195</td>\n",
              "      <td>201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>98</td>\n",
              "      <td>95</td>\n",
              "      <td>46</td>\n",
              "      <td>105</td>\n",
              "      <td>219</td>\n",
              "      <td>68</td>\n",
              "      <td>9</td>\n",
              "      <td>201</td>\n",
              "      <td>33</td>\n",
              "      <td>23</td>\n",
              "      <td>148</td>\n",
              "      <td>223</td>\n",
              "      <td>602</td>\n",
              "      <td>201</td>\n",
              "      <td>69</td>\n",
              "      <td>5</td>\n",
              "      <td>38</td>\n",
              "      <td>191</td>\n",
              "      <td>202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>37</td>\n",
              "      <td>90</td>\n",
              "      <td>48</td>\n",
              "      <td>86</td>\n",
              "      <td>306</td>\n",
              "      <td>126</td>\n",
              "      <td>49</td>\n",
              "      <td>153</td>\n",
              "      <td>44</td>\n",
              "      <td>19</td>\n",
              "      <td>156</td>\n",
              "      <td>272</td>\n",
              "      <td>346</td>\n",
              "      <td>200</td>\n",
              "      <td>118</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>185</td>\n",
              "      <td>194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>563</td>\n",
              "      <td>89</td>\n",
              "      <td>42</td>\n",
              "      <td>75</td>\n",
              "      <td>140</td>\n",
              "      <td>55</td>\n",
              "      <td>6</td>\n",
              "      <td>145</td>\n",
              "      <td>46</td>\n",
              "      <td>19</td>\n",
              "      <td>139</td>\n",
              "      <td>170</td>\n",
              "      <td>312</td>\n",
              "      <td>166</td>\n",
              "      <td>71</td>\n",
              "      <td>15</td>\n",
              "      <td>26</td>\n",
              "      <td>191</td>\n",
              "      <td>198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>488</td>\n",
              "      <td>82</td>\n",
              "      <td>39</td>\n",
              "      <td>86</td>\n",
              "      <td>140</td>\n",
              "      <td>54</td>\n",
              "      <td>7</td>\n",
              "      <td>153</td>\n",
              "      <td>45</td>\n",
              "      <td>19</td>\n",
              "      <td>134</td>\n",
              "      <td>174</td>\n",
              "      <td>338</td>\n",
              "      <td>139</td>\n",
              "      <td>71</td>\n",
              "      <td>11</td>\n",
              "      <td>18</td>\n",
              "      <td>183</td>\n",
              "      <td>189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>147</td>\n",
              "      <td>91</td>\n",
              "      <td>40</td>\n",
              "      <td>76</td>\n",
              "      <td>171</td>\n",
              "      <td>67</td>\n",
              "      <td>7</td>\n",
              "      <td>149</td>\n",
              "      <td>44</td>\n",
              "      <td>19</td>\n",
              "      <td>135</td>\n",
              "      <td>169</td>\n",
              "      <td>332</td>\n",
              "      <td>144</td>\n",
              "      <td>68</td>\n",
              "      <td>4</td>\n",
              "      <td>17</td>\n",
              "      <td>192</td>\n",
              "      <td>200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     0    1   2    3    4    5   6    7   ...   11   12   13   14  15  16   17   18\n",
              "0   383  100  51  109  224   67   9  217  ...  238  704  206   72   6  18  189  199\n",
              "1    47   85  42   66  122   54   6  148  ...  172  317  174   88   6  14  180  182\n",
              "2   319  102  51   92  194   60   6  220  ...  247  731  209   80   7   7  188  186\n",
              "3   675  100  58  109  230   70  11  226  ...  234  752  207   72   0  13  187  198\n",
              "4   667  110  53  104  223   66  10  211  ...  223  659  210   67   5  16  190  203\n",
              "5   539  109  55   96  191   57   6  241  ...  267  857  242   85   8   9  184  184\n",
              "6   789   90  39   85  160   59   7  163  ...  189  396  158   71   7  13  186  192\n",
              "7   409   86  38   86  175   60   9  170  ...  191  433  138   68   1  28  191  199\n",
              "8   121   90  48   78  142   59  11  160  ...  173  370  185   76  10  11  183  192\n",
              "9   702   96  48   83  177   59   8  171  ...  195  438  196   67  15   0  195  201\n",
              "10   98   95  46  105  219   68   9  201  ...  223  602  201   69   5  38  191  202\n",
              "11   37   90  48   86  306  126  49  153  ...  272  346  200  118   0  15  185  194\n",
              "12  563   89  42   75  140   55   6  145  ...  170  312  166   71  15  26  191  198\n",
              "13  488   82  39   86  140   54   7  153  ...  174  338  139   71  11  18  183  189\n",
              "14  147   91  40   76  171   67   7  149  ...  169  332  144   68   4  17  192  200\n",
              "\n",
              "[15 rows x 19 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-98e7d91d77d65fcf",
          "locked": true,
          "schema_version": 2,
          "solution": false
        },
        "id": "HDzou6vtY0r1",
        "colab_type": "text"
      },
      "source": [
        "Methods `describe` and `info` deliver some useful information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64v0WlzEY0r3",
        "colab_type": "code",
        "outputId": "a1577044-7390-4391-c1a8-02c69c1fe11e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "source": [
        "X_train_pd.describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>549.000000</td>\n",
              "      <td>549.000000</td>\n",
              "      <td>549.000000</td>\n",
              "      <td>549.000000</td>\n",
              "      <td>549.000000</td>\n",
              "      <td>549.000000</td>\n",
              "      <td>549.000000</td>\n",
              "      <td>549.000000</td>\n",
              "      <td>549.000000</td>\n",
              "      <td>549.000000</td>\n",
              "      <td>549.000000</td>\n",
              "      <td>549.000000</td>\n",
              "      <td>549.000000</td>\n",
              "      <td>549.000000</td>\n",
              "      <td>549.000000</td>\n",
              "      <td>549.000000</td>\n",
              "      <td>549.000000</td>\n",
              "      <td>549.000000</td>\n",
              "      <td>549.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>434.041894</td>\n",
              "      <td>93.828780</td>\n",
              "      <td>44.947177</td>\n",
              "      <td>82.131148</td>\n",
              "      <td>168.489982</td>\n",
              "      <td>61.429872</td>\n",
              "      <td>8.517304</td>\n",
              "      <td>168.981785</td>\n",
              "      <td>40.954463</td>\n",
              "      <td>20.602914</td>\n",
              "      <td>148.198543</td>\n",
              "      <td>188.444444</td>\n",
              "      <td>440.979964</td>\n",
              "      <td>174.896175</td>\n",
              "      <td>72.395264</td>\n",
              "      <td>6.584699</td>\n",
              "      <td>12.406193</td>\n",
              "      <td>188.959927</td>\n",
              "      <td>195.686703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>251.083281</td>\n",
              "      <td>8.245094</td>\n",
              "      <td>6.205347</td>\n",
              "      <td>16.007785</td>\n",
              "      <td>33.183388</td>\n",
              "      <td>7.074235</td>\n",
              "      <td>4.131304</td>\n",
              "      <td>33.831264</td>\n",
              "      <td>7.933329</td>\n",
              "      <td>2.640724</td>\n",
              "      <td>14.742644</td>\n",
              "      <td>31.449054</td>\n",
              "      <td>179.600199</td>\n",
              "      <td>32.569440</td>\n",
              "      <td>7.223647</td>\n",
              "      <td>4.920780</td>\n",
              "      <td>8.864596</td>\n",
              "      <td>6.289709</td>\n",
              "      <td>7.600676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>73.000000</td>\n",
              "      <td>33.000000</td>\n",
              "      <td>40.000000</td>\n",
              "      <td>104.000000</td>\n",
              "      <td>47.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>112.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>118.000000</td>\n",
              "      <td>130.000000</td>\n",
              "      <td>184.000000</td>\n",
              "      <td>109.000000</td>\n",
              "      <td>59.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>176.000000</td>\n",
              "      <td>181.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>214.000000</td>\n",
              "      <td>87.000000</td>\n",
              "      <td>40.000000</td>\n",
              "      <td>70.000000</td>\n",
              "      <td>141.000000</td>\n",
              "      <td>57.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>146.000000</td>\n",
              "      <td>33.000000</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>137.000000</td>\n",
              "      <td>168.000000</td>\n",
              "      <td>317.000000</td>\n",
              "      <td>150.000000</td>\n",
              "      <td>68.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>184.000000</td>\n",
              "      <td>190.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>450.000000</td>\n",
              "      <td>93.000000</td>\n",
              "      <td>44.000000</td>\n",
              "      <td>80.000000</td>\n",
              "      <td>167.000000</td>\n",
              "      <td>61.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>157.000000</td>\n",
              "      <td>43.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>146.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>363.000000</td>\n",
              "      <td>173.000000</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>188.000000</td>\n",
              "      <td>197.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>647.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>50.000000</td>\n",
              "      <td>98.000000</td>\n",
              "      <td>195.000000</td>\n",
              "      <td>65.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>46.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>160.000000</td>\n",
              "      <td>217.000000</td>\n",
              "      <td>597.000000</td>\n",
              "      <td>198.000000</td>\n",
              "      <td>75.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>193.000000</td>\n",
              "      <td>201.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>845.000000</td>\n",
              "      <td>117.000000</td>\n",
              "      <td>59.000000</td>\n",
              "      <td>110.000000</td>\n",
              "      <td>306.000000</td>\n",
              "      <td>126.000000</td>\n",
              "      <td>52.000000</td>\n",
              "      <td>265.000000</td>\n",
              "      <td>61.000000</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>188.000000</td>\n",
              "      <td>288.000000</td>\n",
              "      <td>1018.000000</td>\n",
              "      <td>268.000000</td>\n",
              "      <td>127.000000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>41.000000</td>\n",
              "      <td>206.000000</td>\n",
              "      <td>211.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               0           1           2   ...          16          17          18\n",
              "count  549.000000  549.000000  549.000000  ...  549.000000  549.000000  549.000000\n",
              "mean   434.041894   93.828780   44.947177  ...   12.406193  188.959927  195.686703\n",
              "std    251.083281    8.245094    6.205347  ...    8.864596    6.289709    7.600676\n",
              "min      0.000000   73.000000   33.000000  ...    0.000000  176.000000  181.000000\n",
              "25%    214.000000   87.000000   40.000000  ...    5.000000  184.000000  190.000000\n",
              "50%    450.000000   93.000000   44.000000  ...   11.000000  188.000000  197.000000\n",
              "75%    647.000000  100.000000   50.000000  ...   18.000000  193.000000  201.000000\n",
              "max    845.000000  117.000000   59.000000  ...   41.000000  206.000000  211.000000\n",
              "\n",
              "[8 rows x 19 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mb7hs5ihY0r-",
        "colab_type": "code",
        "outputId": "7f81c04d-bf17-4e33-dcde-b2ffc62f768a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        }
      },
      "source": [
        "X_train_pd.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 549 entries, 0 to 548\n",
            "Data columns (total 19 columns):\n",
            "0     549 non-null int64\n",
            "1     549 non-null int64\n",
            "2     549 non-null int64\n",
            "3     549 non-null int64\n",
            "4     549 non-null int64\n",
            "5     549 non-null int64\n",
            "6     549 non-null int64\n",
            "7     549 non-null int64\n",
            "8     549 non-null int64\n",
            "9     549 non-null int64\n",
            "10    549 non-null int64\n",
            "11    549 non-null int64\n",
            "12    549 non-null int64\n",
            "13    549 non-null int64\n",
            "14    549 non-null int64\n",
            "15    549 non-null int64\n",
            "16    549 non-null int64\n",
            "17    549 non-null int64\n",
            "18    549 non-null int64\n",
            "dtypes: int64(19)\n",
            "memory usage: 81.6 KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-be844269be69c387",
          "locked": true,
          "schema_version": 2,
          "solution": false
        },
        "id": "Z-68TfPnY0sG",
        "colab_type": "text"
      },
      "source": [
        "### 2. Machine Learning pipeline\n",
        "Here you are supposed to perform the desired transformations. Please, explain your results briefly after each task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qp5zl8pIY0sI",
        "colab_type": "text"
      },
      "source": [
        "#### 2.0. Data preprocessing\n",
        "* Make some transformations of the dataset (if necessary). Briefly explain the transformations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-a1514aa189a49fca",
          "locked": false,
          "points": 15,
          "schema_version": 2,
          "solution": true
        },
        "id": "chyVHAGlY0sJ",
        "colab_type": "code",
        "outputId": "8b7bbd50-aacc-4a08-883a-e7167f8a4717",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train_pd)\n",
        "X_train_pd = scaler.transform(X_train_pd)\n",
        "\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.20347211,  0.74915439,  0.97631021, ...,  0.63160336,\n",
              "         0.00637699,  0.43631889],\n",
              "       [-1.54289395, -1.07176839, -0.47537464, ...,  0.17995863,\n",
              "        -1.42583687, -1.80236401],\n",
              "       [-0.45860008,  0.99194409,  0.97631021, ..., -0.61041966,\n",
              "        -0.15275788, -1.27561509],\n",
              "       ...,\n",
              "       [ 0.46225244, -0.95037354, -1.12056791, ..., -1.17497558,\n",
              "        -0.78929738, -1.27561509],\n",
              "       [ 0.25496096, -0.70758384, -1.44316454, ...,  0.17995863,\n",
              "        -1.42583687, -1.67067678],\n",
              "       [ 0.84095802, -0.95037354, -0.15277801, ..., -1.40079795,\n",
              "        -0.31189276, -0.48549172]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_ZqUwH_Y0sM",
        "colab_type": "text"
      },
      "source": [
        "#### 2.1. Basic logistic regression\n",
        "* Find optimal hyperparameters for logistic regression with cross-validation on the `train` data (small grid/random search is enough, no need to find the *best* parameters).\n",
        "\n",
        "* Estimate the model quality with `f1` and `accuracy` scores.\n",
        "* Plot a ROC-curve for the trained model. For the multiclass case you might use `scikitplot` library (e.g. `scikitplot.metrics.plot_roc(test_labels, predicted_proba)`).\n",
        "\n",
        "*Note: please, use the following hyperparameters for logistic regression: `multi_class='multinomial'`, `solver='saga'` `tol=1e-3` and ` max_iter=500`.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-1dd5ad5d0845cbbb",
          "locked": false,
          "points": 5,
          "schema_version": 2,
          "solution": true
        },
        "id": "o4BCx-HxY0sN",
        "colab_type": "code",
        "outputId": "db60d483-7bed-4244-d0e2-ed3089d0cdc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "LR = LogisticRegression().fit(X_train_pd, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YxWRudGY0sS",
        "colab_type": "code",
        "outputId": "6c0b71b4-b8b0-40a0-cee2-7469c958fd64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "# You might use this command to install scikit-plot. \n",
        "# Warning, if you a running locally, don't call pip from within jupyter, call it from terminal in the corresponding \n",
        "# virtual environment instead\n",
        "\n",
        "! pip install scikit-plot"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-plot\n",
            "  Downloading https://files.pythonhosted.org/packages/7c/47/32520e259340c140a4ad27c1b97050dd3254fdc517b1d59974d47037510e/scikit_plot-0.3.7-py3-none-any.whl\n",
            "Requirement already satisfied: joblib>=0.10 in /usr/local/lib/python3.6/dist-packages (from scikit-plot) (0.14.1)\n",
            "Requirement already satisfied: scipy>=0.9 in /usr/local/lib/python3.6/dist-packages (from scikit-plot) (1.4.1)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-plot) (3.2.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.6/dist-packages (from scikit-plot) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy>=0.9->scikit-plot) (1.18.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.0->scikit-plot) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.0->scikit-plot) (2.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.0->scikit-plot) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib>=1.4.0->scikit-plot) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.4.0->scikit-plot) (46.0.0)\n",
            "Installing collected packages: scikit-plot\n",
            "Successfully installed scikit-plot-0.3.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGgT0YoTY0sX",
        "colab_type": "text"
      },
      "source": [
        "#### 2.2. PCA: explained variance plot\n",
        "* Apply the PCA to the train part of the data. Build the explaided variance plot. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-c6c614740bce090e",
          "locked": false,
          "points": 10,
          "schema_version": 2,
          "solution": true
        },
        "id": "7Mxx4PcvY0sY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### YOUR CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-0c1fe666f52fe53c",
          "locked": true,
          "schema_version": 2,
          "solution": false
        },
        "id": "RBI11RO6Y0se",
        "colab_type": "text"
      },
      "source": [
        "#### 2.3. PCA trasformation\n",
        "* Select the appropriate number of components. Briefly explain your choice. Should you normalize the data?\n",
        "\n",
        "*Use `fit` and `transform` methods to transform the `train` and `test` parts.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-96ab18d96473ef71",
          "locked": false,
          "points": 5,
          "schema_version": 2,
          "solution": true
        },
        "id": "bFhVkPkTY0se",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### YOUR CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EV8iRFjIY0sk",
        "colab_type": "text"
      },
      "source": [
        "**Note: From this point `sklearn` [Pipeline](https://scikit-learn.org/stable/modules/compose.html) might be useful to perform transformations on the data. Refer to the [docs](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) for more information.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-d28b58a35c94e988",
          "locked": true,
          "schema_version": 2,
          "solution": false
        },
        "id": "kALY4dP1Y0sm",
        "colab_type": "text"
      },
      "source": [
        "#### 2.4. Logistic regression on PCA-preprocessed data.\n",
        "* Find optimal hyperparameters for logistic regression with cross-validation on the transformed by PCA `train` data.\n",
        "\n",
        "* Estimate the model quality with `f1` and `accuracy` scores.\n",
        "* Plot a ROC-curve for the trained model. For the multiclass case you might use `scikitplot` library (e.g. `scikitplot.metrics.plot_roc(test_labels, predicted_proba)`).\n",
        "\n",
        "*Note: please, use the following hyperparameters for logistic regression: `multi_class='multinomial'`, `solver='saga'` and `tol=1e-3`*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-12d53ea45258fa82",
          "locked": false,
          "points": 5,
          "schema_version": 2,
          "solution": true
        },
        "id": "3oRvJF8wY0so",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### YOUR CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-4fbf16c64076e139",
          "locked": true,
          "schema_version": 2,
          "solution": false
        },
        "id": "TtdDNrHZY0st",
        "colab_type": "text"
      },
      "source": [
        "#### 2.5. Decision tree\n",
        "* Now train a desicion tree on the same data. Find optimal tree depth (`max_depth`) using cross-validation.\n",
        "\n",
        "* Measure the model quality using the same metrics you used above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-748ed20b51c67fab",
          "locked": false,
          "points": 15,
          "schema_version": 2,
          "solution": true
        },
        "id": "F9QXRroPY0sv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-9eadd4d8a03ae67a",
          "locked": true,
          "schema_version": 2,
          "solution": false
        },
        "id": "X-uJ8H7cY0s0",
        "colab_type": "text"
      },
      "source": [
        "#### 2.6. Bagging.\n",
        "Here starts the ensembling part.\n",
        "\n",
        "First we will use the __Bagging__ approach. Build an ensemble of $N$ algorithms varying N from $N_{min}=2$ to $N_{max}=100$ (with step 5).\n",
        "\n",
        "We will build two ensembles: of logistic regressions and of decision trees.\n",
        "\n",
        "*Comment: each ensemble should be constructed from models of the same family, so logistic regressions should not be mixed up with decision trees.*\n",
        "\n",
        "\n",
        "*Hint 1: To build a __Bagging__ ensebmle varying the ensemble size efficiently you might generate $N_{max}$ subsets of `train` data (of the same size as the original dataset) using bootstrap procedure once. Then you train a new instance of logistic regression/decision tree with optimal hyperparameters you estimated before on each subset (so you train it from scratch). Finally, to get an ensemble of $N$ models you average the $N$ out of $N_{max}$ models predictions.*\n",
        "\n",
        "*Hint 2: sklearn might help you with this taks. Some appropriate function/class might be out there.*\n",
        "\n",
        "* Plot `f1` and `accuracy` scores plots w.r.t. the size of the ensemble.\n",
        "\n",
        "* Briefly analyse the plot. What is the optimal number of algorithms? Explain your answer.\n",
        "\n",
        "* How do you think, are the hyperparameters for the decision trees you found in 2.5 optimal for trees used in ensemble? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-8fc95a2b206bdae1",
          "locked": false,
          "points": 35,
          "schema_version": 2,
          "solution": true
        },
        "id": "lCjJcFR8Y0s2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88joHk0sY0s7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-241b7691ab44cbfb",
          "locked": true,
          "schema_version": 2,
          "solution": false
        },
        "id": "E-Jml0kdY0tA",
        "colab_type": "text"
      },
      "source": [
        "#### 2.7. Random Forest\n",
        "Now we will work with the Random Forest (its `sklearn` implementation).\n",
        "\n",
        "* * Plot `f1` and `accuracy` scores plots w.r.t. the number of trees in Random Forest.\n",
        "\n",
        "* What is the optimal number of trees you've got? Is it different from the optimal number of logistic regressions/decision trees in 2.6? Explain the results briefly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-888755d0f3d91620",
          "locked": false,
          "points": 15,
          "schema_version": 2,
          "solution": true
        },
        "id": "aICaLS19Y0tC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-99191c0852538d4d",
          "locked": true,
          "schema_version": 2,
          "solution": false
        },
        "id": "WAal0dJ1Y0tI",
        "colab_type": "text"
      },
      "source": [
        "#### 2.8. Learning curve\n",
        "Your goal is to estimate, how does the model behaviour change with the increase of the `train` dataset size.\n",
        "\n",
        "* Split the training data into 10 equal (almost) parts. Then train the models from above (Logistic regression, Desicion Tree, Random Forest) with optimal hyperparameters you have selected on 1 part, 2 parts (combined, so the train size in increased by 2 times), 3 parts and so on.\n",
        "\n",
        "* Build a plot of `accuracy` and `f1` scores on `test` part, varying the `train` dataset size (so the axes will be score - dataset size.\n",
        "\n",
        "* Analyse the final plot. Can you make any conlusions using it? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-e39bc7e7dff61ff9",
          "locked": false,
          "points": 15,
          "schema_version": 2,
          "solution": true
        },
        "id": "iYBxSsDJY0tL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}